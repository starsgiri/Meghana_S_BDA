# PySpark Data Processing Commands for Bank Customer Churn Prediction

# ============================================================================
# STEP 1: Initialize PySpark Session
# ============================================================================

from pyspark.sql import SparkSession
from pyspark.sql.functions import col, when, count, isnan, isnull
from pyspark.sql.types import *

# Create Spark Session
spark = SparkSession.builder \
    .appName("BankCustomerChurnPrediction") \
    .config("spark.driver.memory", "4g") \
    .getOrCreate()

print("Spark Session Created Successfully!")

# ============================================================================
# STEP 2: Load Dataset
# ============================================================================

# Load the CSV file
df = spark.read.csv("dataset/Churn_Modelling.csv", header=True, inferSchema=True)

print("Dataset loaded successfully!")
print(f"Number of rows: {df.count()}")
print(f"Number of columns: {len(df.columns)}")

# ============================================================================
# STEP 3: Initial Data Exploration
# ============================================================================

# Display schema
print("\n=== Dataset Schema ===")
df.printSchema()

# Show first few rows
print("\n=== First 5 Rows ===")
df.show(5)

# Get column names
print("\n=== Column Names ===")
print(df.columns)

# Statistical summary
print("\n=== Statistical Summary ===")
df.describe().show()

# ============================================================================
# STEP 4: Check for Missing Values
# ============================================================================

print("\n=== Missing Values Check ===")
df.select([count(when(isnan(c) | col(c).isNull(), c)).alias(c) for c in df.columns]).show()

# ============================================================================
# STEP 5: Analyze Target Variable (Churn)
# ============================================================================

print("\n=== Churn Distribution ===")
df.groupBy("Exited").count().show()

# Calculate churn rate
total_customers = df.count()
churned_customers = df.filter(col("Exited") == 1).count()
churn_rate = (churned_customers / total_customers) * 100
print(f"Churn Rate: {churn_rate:.2f}%")

# ============================================================================
# STEP 6: Feature Analysis
# ============================================================================

# Analyze categorical features
print("\n=== Geography Distribution ===")
df.groupBy("Geography").count().show()

print("\n=== Gender Distribution ===")
df.groupBy("Gender").count().show()

print("\n=== Number of Products Distribution ===")
df.groupBy("NumOfProducts").count().show()

# Analyze churn by categorical features
print("\n=== Churn by Geography ===")
df.groupBy("Geography", "Exited").count().show()

print("\n=== Churn by Gender ===")
df.groupBy("Gender", "Exited").count().show()

# ============================================================================
# STEP 7: Data Cleaning and Preprocessing
# ============================================================================

# Drop unnecessary columns (RowNumber, CustomerId, Surname)
df_clean = df.drop("RowNumber", "CustomerId", "Surname")

print("\n=== Cleaned Dataset Schema ===")
df_clean.printSchema()

# ============================================================================
# STEP 8: Feature Engineering
# ============================================================================

# Create additional features
from pyspark.sql.functions import when

# Age groups
df_clean = df_clean.withColumn("AgeGroup", 
    when(col("Age") < 30, "Young")
    .when((col("Age") >= 30) & (col("Age") < 45), "Middle")
    .when(col("Age") >= 45, "Senior")
)

# Balance category
df_clean = df_clean.withColumn("BalanceCategory",
    when(col("Balance") == 0, "Zero")
    .when((col("Balance") > 0) & (col("Balance") <= 50000), "Low")
    .when((col("Balance") > 50000) & (col("Balance") <= 100000), "Medium")
    .when(col("Balance") > 100000, "High")
)

# Credit Score Category
df_clean = df_clean.withColumn("CreditScoreCategory",
    when(col("CreditScore") < 600, "Poor")
    .when((col("CreditScore") >= 600) & (col("CreditScore") < 700), "Fair")
    .when((col("CreditScore") >= 700) & (col("CreditScore") < 800), "Good")
    .when(col("CreditScore") >= 800, "Excellent")
)

print("\n=== Dataset with Engineered Features ===")
df_clean.show(5)

# ============================================================================
# STEP 9: Save Processed Data
# ============================================================================

# Save as Parquet for efficient storage and processing
df_clean.write.mode("overwrite").parquet("dataset/processed_churn_data.parquet")

# Also save as CSV for reference
df_clean.coalesce(1).write.mode("overwrite") \
    .option("header", "true") \
    .csv("dataset/processed_churn_data_csv")

print("\n=== Processed data saved successfully! ===")

# ============================================================================
# STEP 10: Final Statistics
# ============================================================================

print("\n=== Final Dataset Statistics ===")
print(f"Total Records: {df_clean.count()}")
print(f"Total Features: {len(df_clean.columns)}")
print(f"\nFeature List: {df_clean.columns}")

# Display correlations for numeric features
print("\n=== Feature Correlations with Churn ===")
numeric_features = ["CreditScore", "Age", "Tenure", "Balance", 
                   "NumOfProducts", "HasCrCard", "IsActiveMember", "EstimatedSalary"]

for feature in numeric_features:
    correlation = df_clean.stat.corr(feature, "Exited")
    print(f"{feature}: {correlation:.4f}")

print("\n=== Data Processing Complete! ===")

# Stop Spark Session (optional - use when done with all processing)
# spark.stop()
